# CODE AIRFLOW : Extraction, prÃ©-traitement et Chargement dans Big Query


import pandas as pd
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime
import os
from google.oauth2 import service_account
from airflow.providers.http.hooks.http import HttpHook 
import json


# ðŸ“Œ Configurations
BQ_PROJECT_ID = "cart-trend-projet"
BQ_DATASET_ID = "CartTrend"
DRIVE_FOLDER_ID = "1ydYmJnSiL3EKE_m7Zr8qauwj4E4aAn-i"

FILES_TO_TABLES = {
    "Carttrend_Commandes": "Carttrend_Commandes",
    "Carttrend_Produits": "Carttrend_Produits",
    "Carttrend_Clients": "Carttrend_Clients",
    "Carttrend_Details_Commandes": "Carttrend_Details_Commandes",
    "Carttrend_Entrepots": "Carttrend_Entreprots",
    "Carttrend_Entreprots_Machines": "Carttrend_Entreprots_Machines",
    "Carttrend_Campaigns": "Carttrend_Campaigns",
    "Carttrend_Posts": "Carttrend_Posts",
    "Carttrend_Satisfaction": "Carttrend_Satisfaction",
    "Carttrend_Promotions": "Carttrend_Promotions",
}

mappage_colonnes = {
    "temps_d'arrÃªt": 'temps_darrÃªt',
}

def extract_from_drive(ti):
    extracted_data = []
    files_list = [
        {'name': 'Carttrend_Entrepots', 'url': 'https://docs.google.com/spreadsheets/d/1FSP2Gv31H1lnpLh6nmaNFcKlCE11OlbA/edit'},
        {'name': 'Carttrend_Entreprots_Machines', 'url': 'https://docs.google.com/spreadsheets/d/1s9R6eJPlC0Vwz_OPRTZ43XXfknBAXktn/edit'},
        {'name': 'Carttrend_Produits', 'url': 'https://docs.google.com/spreadsheets/d/1I4KHaFSEMMJ2E7OEO-v1KWbYfOGUBGiC8XCUVvFHs2I/edit'},
        {'name': 'Carttrend_Commandes', 'url': 'https://docs.google.com/spreadsheets/d/1QVXmhf9b2OSpUVb7uBOQOClk19ldleNYQcloKCrHlgA/edit'},
        {'name': 'Carttrend_Posts', 'url': 'https://docs.google.com/spreadsheets/d/1N81drG9zhp9VBZh3LqPoQ01cMvXol1kX43hqhQtAZ44/edit'},
        {'name': 'Carttrend_Promotions', 'url': 'https://docs.google.com/spreadsheets/d/1p2O-Zgmhcmfov1BkLb7Rx9k2iwg65kFcgVyYwb4CYs4/edit'},
        {'name': 'Carttrend_Satisfaction', 'url': 'https://docs.google.com/spreadsheets/d/1G7rST778z_zcewJX9CuURwIqTSKfWCU_i6ZJ9P8edzM/edit'},
        {'name': 'Carttrend_Details_Commandes', 'url': 'https://docs.google.com/spreadsheets/d/1kN4O2D-LIvbLSTse2RsguJMPwdMWKtVY6dEl_4hcyqw/edit'},
        {'name': 'Carttrend_Clients', 'url': 'https://docs.google.com/spreadsheets/d/1PkZuSLHn0eZQLjhBx8qdZ_bh_wzgMbenrYyMGYrxBic/edit'},
        {'name': 'Carttrend_Campaigns', 'url': 'https://docs.google.com/spreadsheets/d/1_WxFdSWGGCNreMgSWf9nfuP-Ye_RnCX1Xs5ubnjGp9s/edit'},
    ]

    for file in files_list:
        url = file["url"]
        file_name = file["name"]
        csv_url = url.replace("/edit", "/export?format=csv")

        try:
            df = pd.read_csv(csv_url)
            extracted_data.append((file_name, df))
            print(f"âœ… {file_name} chargÃ© ({df.shape[0]} lignes, {df.shape[1]} colonnes)")
        except Exception as e:
            print(f"âŒ Erreur avec {file_name}: {e}")

    ti.xcom_push(key='extracted_data', value=extracted_data)

def renommer_colonnes(ti):
    extracted_data = ti.xcom_pull(task_ids='extract_data', key='extracted_data')
    transformed_data = {}

    for file_name, df in extracted_data:
        df.columns = df.columns.str.strip()
        df.columns = df.columns.str.replace("â€™", "'", regex=False)
        df.rename(columns=mappage_colonnes, inplace=True)
        transformed_data[file_name] = df

    ti.xcom_push(key='transformed_data', value=transformed_data)

def load_to_bigquery(ti):
    transformed_data = ti.xcom_pull(task_ids='renommer_colonnes', key='transformed_data')

    if not transformed_data:
        print("âš ï¸ Aucune donnÃ©e transformÃ©e trouvÃ©e, arrÃªt du chargement vers BigQuery.")
        return

    credentials = service_account.Credentials.from_service_account_file("/root/airflow/cart-trend-projet.json") ## ðŸ“ŒðŸ“Œ Changement de votre clef JSON et le Chemin ðŸ“ŒðŸ“Œ ##

    for file_name, df in transformed_data.items():
        if df is None or df.empty:
            print(f"âš ï¸ Aucune donnÃ©e Ã  charger pour {file_name}, passage au suivant.")
            continue

        if file_name in FILES_TO_TABLES:
            table_name = FILES_TO_TABLES[file_name]
            print(f"ðŸ“¤ Remplacement des donnÃ©es de {file_name} vers BigQuery ({BQ_DATASET_ID}.{table_name})")
            df.to_gbq(
                f"{BQ_DATASET_ID}.{table_name}",
                project_id=BQ_PROJECT_ID,
                if_exists="replace",
                credentials=credentials
            )

# ðŸ“Œ DBT Execution via API (Utilisation de HttpHook au lieu de SimpleHttpOperator)
DBT_CLOUD_ACCOUNT_ID = "70471823437194"
DBT_CLOUD_JOB_ID = "70471823434709"

def trigger_dbt_cloud_run():
    http = HttpHook(method="POST", http_conn_id="dbt_cloud")

    payload = {
        "cause": "Triggering from Airflow"
    }

    # Convertir le dictionnaire en JSON
    json_payload = json.dumps(payload)

    response = http.run(
        endpoint=f"/api/v2/accounts/{DBT_CLOUD_ACCOUNT_ID}/jobs/{DBT_CLOUD_JOB_ID}/run/",
        headers={"Content-Type": "application/json"},
        data=json_payload  # Passer la chaÃ®ne JSON ici
    )
    return response.text

dbt_auto_data = PythonOperator(
    task_id="dbt_auto_data",
    python_callable=trigger_dbt_cloud_run
)

# ðŸ“Œ DÃ©finition du DAG
default_args = {
    "owner": "airflow",
    "start_date": datetime(2024, 3, 1),
    "retries": 1
}

with DAG(
    dag_id="google_drive_to_bigquery_and_dbt",
    default_args=default_args,
    schedule_interval="@daily",
    catchup=False
) as dag:

    extract_data = PythonOperator(
        task_id="extract_data",
        python_callable=extract_from_drive,
        provide_context=True
    )

    renommer_colonnes_task = PythonOperator(
        task_id="renommer_colonnes",
        python_callable=renommer_colonnes,
        provide_context=True
    )

    load_data = PythonOperator(
        task_id="load_data",
        python_callable=load_to_bigquery,
        provide_context=True
    )

    extract_data >> renommer_colonnes_task >> load_data >> dbt_auto_data

